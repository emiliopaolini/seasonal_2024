{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Jupyter Notebooks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning** has recently allowed to achieve outstanding performance in several domains, ranging from speech recognition to visual object detection and recognition.\n",
    "Deep *convolutional neural networks* have proven effective in processing images, video, speech and audio, whereas *recurrent neural networks* are particularly suited for sequential data such as text and speech. \n",
    "\n",
    "Deep Neural Networks, with *many* layers of information processing between input and output, are able to represent data with multiple levels of abstraction. Training such deep architectures in reasonable time was made possible in the last decade by the advent of fast graphic processing units (**GPU**)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"**Notebook**\" term may refer both to the notebook document and the web-based environment used to create it. Jupyter Notebook can connect to an **IPython kernel** to allow interactive programming in Python. It uses an internal library for converting the document in HTML and allows visualization and editing in the browser.\n",
    "\n",
    "The document you are reading, with extension .ipynb, is a notebook and consists in an ordered **list of cells** which can contatin code, text, mathematics, visualization of output and plots."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **key features** of a Jupyter Notebook are the following:\n",
    "\n",
    "- it consists in *executable Python code* enriched with *text-editing capabilities*;\n",
    "- it allows interactive development: small pieces of code (cells) can be executed independently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two types of Cells\n",
    "A notebook is a list of cells. There are two types of cell:\n",
    "- Code cells:  contain executable code.\n",
    "- Markdown cells: contain explanatory text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code cell\n",
    "A code cell contains executable code and displays its output just below.\n",
    "The subsequent cells are examples of code cell: execute them clicking the play button or using Ctrl+Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an executable code cell. \n",
    "\n",
    "# first python snippet\n",
    "a = 2\n",
    "b = 4\n",
    "a*b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*System* aliases: use exclamation mark for terminal operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter includes shortcuts for common operations\n",
    "!which python\n",
    "!python --version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text cell \n",
    "This is a **text cell**! Text cells use *markdown syntax*:  it consists in plain text formatting syntax that enables the creation of rich text that can be converted to HTML!\n",
    "\n",
    "You can include well-formatted text, formulas, and fancy images too!\n",
    "\n",
    "![image logo](https://prod-discovery.edx-cdn.org/media/course/image/493b81e0-eb2e-4a41-acf4-dc39273c16cf-006892747a54.small.jpg)\n",
    "\n",
    "To learn more, see  the [markdown guide](/notebooks/markdown_guide.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook and IPython Kernel\n",
    "Several interfaces (e.g. Notebooks) use the IPython Kernel. The IPython Kernel is a separate process which is responsible for running user code, and things like computing possible completions. Notebook frontend communicate with the IPython Kernel using JSON messages sent over the sockets provided by a messagging library named ZeroMQ. Sockets are objects for sending and receiving data between local processes (*unix domain sockets*) or in a computer network (*network sockets*). \n",
    "\n",
    "WebSocket is a communication protocol that allow client-server communication in both directions and simultaneously.\n",
    "\n",
    "![notebook image](https://jupyter.readthedocs.io/en/latest/_images/notebook_components.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook files\n",
    "Furthermore, the Notebook frontend stores code and output, together with markdown notes, in an editable document called a notebook. \n",
    "\n",
    "When you save it, this is sent from your browser to the notebook server, which saves it on disk as a JSON file with a .ipynb extension. JSON stands for JavaScript Object Notation: it is an open-standard file format that uses human-readable text to transmit data objects consisting of attribute-value pairs and array data types. Try to download this notebook and look at it with a text editor!\n",
    "\n",
    "```\n",
    "{\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 0,\n",
    "  \"metadata\": {\n",
    "    \"colab\": {\n",
    "      \"name\": \"notebook.ipynb\",\n",
    "      \"version\": \"0.3.2\",\n",
    "      \"provenance\": [],\n",
    "      \"collapsed_sections\": [],\n",
    "      \"toc_visible\": true\n",
    "    },\n",
    "    \"kernelspec\": {\n",
    "      \"name\": \"python3\",\n",
    "      \"display_name\": \"Python 3\"\n",
    "    }\n",
    "  },\n",
    "  \"cells\": [\n",
    "   ...\n",
    "   ]\n",
    "   ...\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "The notebook server, not the kernel, is responsible for saving and loading notebooks, so you can edit notebooks even if you don’t have the kernel for that language—you just won’t be able to run code. The kernel doesn’t know anything about the notebook document: it just gets sent cells of code to execute when the user runs them.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "\n",
    "Tensorflow is an open-source software library, written in Python and C++, used for expressing and executing machine learning algorithms. It was developed by Google Brain and released on November 2015.\n",
    "\n",
    "Abstract from **Abadi, Martín, et al. 2015**:\n",
    "\n",
    "*TensorFlow is **an interface for expressing machine learning algorithms, and an implementation for executing such algorithms**. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from **mobile devices** such as phones and tablets up to **large-scale distributed systems** of hundreds of machines and thousands of computational devices such as **GPU cards**. The system is flexible and can be used to express a wide variety of algorithms, including **training and inference algorithms for deep neural network models**, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including **speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery**. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an **open-source package** under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org*\n",
    "\n",
    "On September 30th 2019, **Tensorflow 2.0** has been released: according to the authors it is an easier-to-use, more flexible and powerful platform for the development and the deployment of Machine Learning applications.\n",
    "Read the Tensorflow [blog post](https://medium.com/tensorflow/tensorflow-2-0-is-now-available-57d706c2a9ab) for further information.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "- https://keras.io/\n",
    "\n",
    "\n",
    "Keras is a **high-level API** to build and train deep learning models and it is **capable of running on Tensorflow**, CNTK and Theano. \n",
    "   \n",
    "\n",
    "API stands for \"Application Programming Interface.\"; in general, it is a set of *commands*, *functions*, *protocols*, and *objects* that programmers can use to create software or interact with an external system. It makes it easier to develop a computer program by providing all the building blocks.\n",
    "\n",
    "\n",
    "Keras, in particular, is used for fast prototyping, advanced research, and production, with three key advantages:\n",
    "- *User friendly*: Keras has a simple, consistent interface optimized for common use cases. It provides clear and actionable feedback for user errors.\n",
    "- *Modular and composable*: Keras models are made by connecting configurable building blocks together, with few restrictions.\n",
    "- *Easy to extend*: Write custom building blocks to express new ideas for research. Create new layers, loss functions, and develop state-of-the-art models.\n",
    "\n",
    "Furthermore:\n",
    "- it allows the same code to run seamlessly on CPU and GPU\n",
    "- it has built-in support for Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN)\n",
    "- it supports arbitrary network architectures (functional API)\n",
    "\n",
    "### Easy prototyping of Deep Learning models\n",
    "Typical workflow for developing with Keras:\n",
    "\n",
    "1. Define training data (input tensor and target tensor)\n",
    "2. Define a model (network as a combination of layers) that maps input tensors to target tensors\n",
    "3. Configure the learning process by choosing a loss function, an optimizer, and a metric to monitor\n",
    "4. Train your model on your data\n",
    "5. Eventually, test your model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.keras\n",
    "\n",
    "- https://www.tensorflow.org/guide/keras\n",
    "\n",
    "One of the main novelties of Tensorflow 2.0 is the tight integration of Keras into Tensorflow. Actually, the first integration of keras as a submodule in Tensorflow dates back to Tensorflow 1.10.0\n",
    "\n",
    "Instead of resorting to an external library (`keras`) we can directly refer to a tensorflow module (`tf.keras`) that implements the same Keras high-level API for Tensorflow.\n",
    "\n",
    "\n",
    "Release notes for Keras 2.3.0 on September 17th, 2019, by Francois Chollet  (Keras creator):\n",
    "\n",
    "\n",
    "- ***This release brings the API in sync with the tf.keras API as of TensorFlow 2.0.*** *However note that it does not support most TensorFlow 2.0 features, in particular eager execution. If you need these features, use tf.keras.*\n",
    "\n",
    "- *This is also the last major release of multi-backend Keras. Going forward, **we recommend that users consider switching their Keras code to tf.keras in TensorFlow 2.0**.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Basics\n",
    "## Key Features\n",
    "- Writing down “readable” code\n",
    "  - Natural syntax\n",
    "  - “Blocks by Indentation” forces proper code structuring & readability:\n",
    "     - **the whitespace indentation of a piece of code affects its meaning. The statements of a logical block should all have the same indentation. If one of the lines in a group has a different indentation, it is flagged as a syntax error.**\n",
    "     - According to the official Python style guide (PEP 8), you should indent with 4 spaces. Google's (and indeed **Colab**'s) internal style guideline dictates indenting by 2 spaces.\n",
    "- Code reuse\n",
    "  - Straightforward, flexible way to use modules (libraries)\n",
    "  - Massive amount of libraries freely available\n",
    "- Object-oriented programming\n",
    "  - OO structuring: effective in tackling complexity for large programs\n",
    "- High performance (close ties to C)\n",
    "  - NumPy (numerical library) allows fast matrix algebra\n",
    "  - Can dump time-intensive modules in C easily"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** No type declaration is required in Python: **\"If it walks like a duck, and it quacks like a duck, then we would call it a duck”**\n",
    "- Type info is associated with objects, not with referencing variables!\n",
    "- This is “duck typing”, widely used in scripting languages.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Data Types\n",
    "\n",
    "### Numerics\n",
    "  - Integers\n",
    "  - Floating-Point numbers\n",
    "  - Complex Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1.0 # assignment statement\n",
    "print(type(x)) # print statement\n",
    "print(id(x)) # memory address\n",
    "z = 33\n",
    "print(type(z)) \n",
    "print(id(z))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text sequence type\n",
    "  - strings: you can define a string by enclosing it with double quotes (\") or single quotes (') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"foobar\"\n",
    "print(type(a))\n",
    "print(len(a))  # returns the length of the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0.982\n",
    "error_rate = 0.118\n",
    "\n",
    "# %-formatting\n",
    "print('accuracy: %.3f \\t error_rate: %.3f' % (accuracy,error_rate))\n",
    "# str.format\n",
    "print('accuracy: {acc} \\t error_rate: {err}'.format(acc=accuracy,err = error_rate))\n",
    "# f-string\n",
    "print(f'accuracy: {accuracy} \\t error_rate: {error_rate}')\n",
    "# f-string: specify decimal precision\n",
    "print(f'accuracy: {accuracy:.2} \\t error_rate: {error_rate:.2}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing operation on string. Zero-based indexing and negative numbers:\n",
    "\n",
    "![indexing](https://developers.google.com/edu/python/images/hello.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'Hello'\n",
    "print(word[1])     # python is 0-based indexing\n",
    "print(word[0:3])   # python is 0-based indexing\n",
    "print(word[:3])    # you can omit the first index when it is 0 \n",
    "print(word[2:])    # you can omit the second index when you mean the end of the string \n",
    "print(word[2:5])  \n",
    "print(word[2:100]) # too big index: truncated to the string length\n",
    "print(word[2:-1])  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many others data types:\n",
    " - tuples (immutable collections)\n",
    " - dictionaries\n",
    " - classes,\n",
    " - instances, \n",
    " - exceptions\n",
    " - boolean values\n",
    " - ...\n",
    "\n",
    "Objects of built-in types like **list**, **set**, **dict** are **mutable**. Custom classes are generally mutable.\n",
    "\n",
    "Objects of built-in types like **int**, **float**, **bool**, **str**, **tuple** are **immutable**.\n",
    "\n",
    "Take a look at this tool: https://pythontutor.com/visualize.html#mode=edit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first look at Classes\n",
    "- from [python docs](https://docs.python.org/3/tutorial/classes.html)\n",
    "\n",
    "The following *dog class* is just an example to get familiar with Object Oriented Programming and the notation using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog:\n",
    "  \"\"\"A simple example class\"\"\"\n",
    "  \n",
    "  def __init__(self, name = 'Bau'): # method for initializing a new instance\n",
    "    # self keyword refers to the newly initialized object\n",
    "    # data attributes\n",
    "    self.name = name    # instance attribute, unique to each instance \n",
    "    self.tricks = []    # creates a new empty list for each dog \n",
    " \n",
    "  def add_trick(self, trick): # a method\n",
    "    self.tricks.append(trick)\n",
    "\n",
    "    \n",
    "c = Dog()\n",
    "print(c.name)\n",
    "d = Dog('Fido')\n",
    "e = Dog('Buddy')\n",
    "c.add_trick('just bark')\n",
    "d.add_trick('bark')\n",
    "d.add_trick('roll over')\n",
    "print(c.tricks)\n",
    "print(d.tricks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a Deep Learning model in Keras is not that different:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```python\n",
    "network = Sequential()  \n",
    "# We created an instance of a Sequential model: it allows to stack layers\n",
    "network.add(Dense(512, input_shape=(28 * 28,))) \n",
    "# Dense is a class that extends the Layer class. We created an instance and added it to our network\n",
    "network.add(Dense(512)\n",
    "# We just added another layer\n",
    "network.add(Dense(10))\n",
    "# We created another instance of Dense layer with a different number of units: it will be the output layer of our network\n",
    "network.compile(optimizer, loss,metric)\n",
    "# The compile method configures the model for training\n",
    "network.fit(train_images, train_labels)\n",
    "# The fit method trains the model on our training data\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules and Packages\n",
    "A **module** is a .py file containing functions/variables\n",
    "- *functions* allow reuse of code *within* program;\n",
    "- *modules* allow reuse of code *across* program;\n",
    "\n",
    "The Python Standard Library is a collection of modules that provides implementations of common facilities such as access to the operating system, file I/O and many others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # module for operating system functionalities\n",
    "os.listdir()\n",
    "os.makedirs('example')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files and Utilities\n",
    "The *os* module include many functions to interact with the file system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'example'\n",
    "if not os.path.exists(out_dir):# check if a directory already exists...\n",
    "  os.makedirs(out_dir)         # ... otherwise create it\n",
    "\n",
    "a = range(100)\n",
    "b = [x*x for x in a]\n",
    "file_path = os.path.join(out_dir,'prova.csv') # Join one or more path components intelligently\n",
    "\n",
    "mf =  open(file_path,'w') # open the file in writing mode\n",
    "for x in zip(a,b):\n",
    "  mf.write(str(x[0])+','+str(x[1])+'\\n')\n",
    "mf.close()  # close the file!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = open(file_path, 'r')\n",
    "for line_number,line in enumerate(mf):  ## iterates over the lines of the file\n",
    "  print(line)    \n",
    "  if line_number==10:\n",
    "    break\n",
    "mf.close()  # close the file!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a preferable sintax (both for writing and reading): file is closed automatically\n",
    "with open(file_path,'r') as mf:\n",
    "  for number,x in enumerate(mf):\n",
    "    print(x)\n",
    "    if number==10:\n",
    "      break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy & Matplotlib\n",
    "- Recommended reading: \n",
    "  - [stanford Class: Python Numpy Tutorial](http://cs231n.github.io/python-numpy-tutorial/)\n",
    "  - [Numpy QuickStart Tutorial](https://numpy.org/doc/stable/user/quickstart.html)\n",
    "  - [Numpy for Matlab Users](https://numpy.org/doc/stable/user/numpy-for-matlab-users.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy (**Num**erical **Py**thon) is the fundamental package for scientific computing with Python. It contains, among other things:\n",
    "\n",
    "- a powerful N-dimensional array object\n",
    "- sophisticated functions that support broadcasting (i.e. it allows to perform arithmetic operations between arrays with different shape)\n",
    "- tools for integrating C/C++ and Fortran code\n",
    "- useful linear algebra, Fourier transform, and random number capabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ndarray**: N-dimensional Array. It represents a multidimensional, homogeneous array of fixed-size items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statements for the import of numpy and matplotlib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0.0, 100, 0.1) # return evenly spaced (step) values within the interval [start, stop)\n",
    "print(type(t))\n",
    "print(len(t))\n",
    "print(t.shape)\n",
    "print(id(t))\n",
    "t[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[10]= 0 # ndarray are mutable objects\n",
    "print(id(t))\n",
    "t[:15]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should get familiar with the concept of **tensor**: \n",
    "it is a generalization of vector and matrices, with an arbitrary number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that prints all properties of a tensor\n",
    "def print_properties(my_tensor):\n",
    "  print('value')\n",
    "  print(my_tensor)\n",
    "  print('properties')\n",
    "  print(f'\\tshape: {my_tensor.shape}') \n",
    "  print(f'\\tnumber of axis: {my_tensor.ndim}')\n",
    "  print(f'\\ttype of elements: {my_tensor.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar: 0D tensor\n",
    "my_scalar = np.array(7)\n",
    "print_properties(my_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector: 1 axis - 1D tensor\n",
    "list_of_numbers = [2,3,5,6,1,-23]\n",
    "my_vector = np.array(list_of_numbers)\n",
    "print_properties(my_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrices: 2 axes - 2D tensor\n",
    "my_matrix = np.random.random((4,3))\n",
    "print_properties(my_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors for Neural Networks \n",
    "We will typically have to deal with even higher dimensional tensor. We will not train a Neural Network with a single image at a time!\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images,train_labels),(test_images,test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_properties(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use matplotlib to plot the first training sample:\n",
    "plt.imshow(train_images[0,:,:],cmap = 'gray')\n",
    "plt.title('first sample')\n",
    "print(f'true label is {train_labels[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we deal with RGB images (3 channels) we need also the channel axis. Tensor would be 4-Dimensional: `(samples, height, width, channel) `\n",
    "\n",
    "When we deal with video data we need also the frame axis. Tensor would be 5-Dimensional: `(samples, frame, height, width, channel) `"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common tensor operation: **dot product**.\n",
    "\n",
    "\n",
    "> Note that unlike MATLAB, `*` is elementwise multiplication, not matrix multiplication. We instead use the dot function to compute inner products of vectors, to multiply a vector by a matrix, and to multiply matrices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.rand(2,3)\n",
    "y = np.random.rand(3,2)\n",
    "z = np.dot(x,y)\n",
    "\n",
    "# matrix multiplication\n",
    "for tensor in [x,y,z]:\n",
    "  print(tensor.shape)\n",
    "  print(tensor)\n",
    "\n",
    "# scalar product\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([100,10,1])\n",
    "c = np.dot(a,b)\n",
    "print(c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![layer](https://www.jeremyjordan.me/content/images/2018/01/Screen-Shot-2017-11-07-at-12.32.19-PM.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shape** of tensors matters. What if I want to reshape an existing tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(2,3)\n",
    "\n",
    "z = x.reshape((6,))\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(z.shape)\n",
    "print(z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib\n",
    "recommended reading: \n",
    "- [Sample plots in Matplotlib](https://matplotlib.org/2.1.2/tutorials/introductory/sample_plots.html)\n",
    "- [colab notebook](https://colab.research.google.com/notebooks/charts.ipynb?authuser=1) about using Matplotlib and other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0,10,0.1)\n",
    "sin_t = np.sin(2*np.pi*t/5)\n",
    "cos_t = np.cos(2*np.pi*t/5)\n",
    "\n",
    "plt.plot(t,sin_t)\n",
    "plt.title('Sin function')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"img\"):\n",
    "  os.makedirs(\"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t,cos_t,'.r', label = 'cosine')\n",
    "plt.plot(t,sin_t,'--g', label = 'sin')\n",
    "plt.title('two functions')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('values')\n",
    "plt.ylim([-1.1,1.5])\n",
    "plt.legend(loc='upper center')\n",
    "\n",
    "plt.savefig('img/example_figure_1.png', format = 'png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two subplots, the axes array is 1-d\n",
    "f, axarr = plt.subplots(2, sharex=True)\n",
    "axarr[0].plot(t,cos_t,'.r')\n",
    "axarr[1].plot(t,sin_t,'.b')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two subplots, the axes array is 1-d\n",
    "f, axarr = plt.subplots(nrows = 1, ncols = 2, sharey=True)\n",
    "axarr[0].plot(t,cos_t,'.r')\n",
    "axarr[1].plot(t,sin_t,'.b')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.random.randn(100000)\n",
    "f, axarr = plt.subplots(1,2)\n",
    "axarr[0].hist(n)\n",
    "axarr[1].hist(n,bins = 100)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to save a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_name = \"img\"\n",
    "if not os.path.exists(dir_name):\n",
    "  os.makedirs(dir_name)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t,sin_t,'--',color = '#556700', label = 'sin')\n",
    "plt.title('two functions')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('values')\n",
    "plt.plot(t,cos_t,'.r', label = 'cosine')\n",
    "plt.grid(axis = 'y')\n",
    "plt.ylim([-1.1,1.1])\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "\n",
    "plt.savefig(os.path.join(dir_name,'example_figure_2.png'), format = 'png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Basics: core components of Neural Networks\n",
    "\n",
    "- Goodfellow, I., Bengio, Y., Courville, A. (2016). Deep learning (Vol. 1). Cambridge: MIT press. ([available online](https://www.deeplearningbook.org/)).\n",
    "- Chapter 3, Section 1 of [Deep Learning with Python, 1st edition](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). \n",
    "- [Stanford Class CS231n](http://cs231n.github.io/neural-networks-1/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep learning** is a branch of machine learning and consists in a\n",
    "set of algorithms and techniques for learning in deep neural networks.\n",
    "\n",
    "**Deep Neural Networks** consists of *many* hidden layers of information processing, in contrast with *shallow* Neural Networks. \n",
    "\n",
    "Here is an illustration reporting toy models of *shallow* and *deep* feedforward Neural Networks \n",
    "![non_deep_vs_deep](https://i.stack.imgur.com/OH3gI.png)\n",
    "\n",
    "NNs key feature is the weighted interconnection of several simple Action Units in order to calculate complex functions\n",
    "\n",
    "Feed-Forward neural networks represent a **universal approximation framework**: in fact, the *universal approximation theorem* states that:\n",
    "\n",
    ">*for a big enough neural network, it always exists a parameters configuration that makes the network able to approximate any continuous function defined on a compact set of the nth-dimensional vector space over the real numbers.*\n",
    "\n",
    "\n",
    "Obviously, it is not guaranteed that the training procedure will guide the\n",
    "model to that parameters configuration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological Neuron\n",
    "<img src=\"http://cs231n.github.io/assets/nn1/neuron.png\" width=\"400\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Model / Artificial Neuron\n",
    "<img src=\"http://cs231n.github.io/assets/nn1/neuron_model.jpeg\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Units: Activation Functions\n",
    "\n",
    "The mathematical neuron model is a coarse approximation of the biological neuron. Its activation function takes the **weighted sum** of the inputs and generates an output signal.\n",
    "\n",
    "-  an action unit receives input signal $x_i$ for each $i$ preceding unit;\n",
    "- the output $y_k$ of the neuron $k$ is a non linear function $f$ of its weighted input:\n",
    "$$y_k = f\\Big(\\sum_i(w_i\\cdot x_i)+b\\Big)$$\n",
    "\n",
    "- the weight $w$ represents the strength of the synaptic connection between two action units. It is a learnable parameter and can result in an excitatory effect (positive weight) or inhibitory effect (negative weight);\n",
    "- the bias $b$ is equivalent to a threshold;\n",
    "- $f$ represents the activation function. This non-linearity allows a neural network to compute non linear functions of its input. The most common functions used are: Sigmoid, Hyperbolic Tangent, ReLU (Rectified Linear Unit);\n",
    "- the output of a unit represents an input for following units.\n",
    "\n",
    "A good activation function has several properties:\n",
    "\n",
    "1. it is **continuous** and has an **infinite domain**, so to have an output number for *any* input;\n",
    "2. it is **monotonic**. With a parabola, for example, we may have the same output value for two different input values;\n",
    "3. it is **non-linear**: a NN with plenty of layers, but using only linear activation functions is equivalent to a NN with only one linear layer;\n",
    "4. it should be **efficiently computable**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function characterizes the Neuron Unit. Some popular examples of activation function are presented in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from tensorflow.python.framework import ops\n",
    "# snippets from https://github.com/nfmcclure/tensorflow_cookbook/blob/master/01_Introduction/06_Implementing_Activation_Functions/06_activation_functions.ipynb\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "x_vals = np.linspace(start=-10., stop=10., num=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step Function**: The trivial step function is seldom used in practice:\n",
    "during the learning procedure it is desirable that small changes in input determine small changes in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(x_vals,[1 if x>0 else 0 for x in x_vals ], label='threshold', linewidth=2)\n",
    "plt.ylim([-1,2])\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid** guarantees the above mentioned property: output values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sigmoid = sess.run(tf.nn.sigmoid(x_vals))\n",
    "plt.plot(x_vals, y_sigmoid, label='Sigmoid', linewidth=2)\n",
    "plt.ylim([-2,2])\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tanh** is similar to sigmoid, but its output is zero-centered: produces values between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tanh = sess.run(tf.nn.tanh(x_vals))\n",
    "plt.plot(x_vals, y_tanh, label='Tanh', linewidth=2)\n",
    "plt.ylim([-2,2])\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLU** (Rectified Linear Unit) is the most popular choice for many deep architectures (mainly Convolutional Neural Networks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_relu = sess.run(tf.nn.relu(x_vals))\n",
    "plt.plot(x_vals, y_relu, label='ReLU', linewidth=2)\n",
    "plt.ylim([-10,10])\n",
    "plt.xlim([-10,10])\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x)=\\max(0,x)$,\n",
    "- it does not require expensive computation, by only thresholding the activations at zero; \n",
    "- it allows to avoid *vanishing gradient problem*: it is a phenomenon by which the gradient tends to get smaller as we move backward through the hidden layers. As a consequence, the update term for neurons in earlier layers will tend to zero, and learning will be very slow. The reason behind this phenomenon lays in the shape of the derivative of the activation function. Saturation of sigmoid units takes place both for small and big input, and indeed its derivative tends to zero. Thanks to the shape of its derivative, ReLU activation allows avoiding this problem.  \n",
    "- it allows a sparse representation that, in turn, guarantees \n",
    "  - information disentangling: small changes of the input do not affect the features representation, as is the case of a dense representation.  A dense representation is highly entangled because almost any change in the input modifies most of the entries in the representation vector. Instead, if a representation is both sparse and robust to small input changes, the set of non-zero features is almost always roughly conserved by small changes of the input.\n",
    "  - efficient variable-size representation: the model itself can reduce its representational power, i.e. the number of active neurons, depending on the input;\n",
    "  - easier linear separability of representation,  because the information is represented in a high-dimensional space. \n",
    "- A drawback of these units is the fact that they do not learn when they have negative input: when too many units lie in this region, the effective capacity of the model could be dramatically reduced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The desired distribution in the output layer guides the choice of the output units.\n",
    "\n",
    "For Multinoulli output distribution (n-way classification problem), softmax units are chosen. \n",
    "Given the input $z = wx + b$, the softmax activation is computed as:\n",
    "\n",
    "  \n",
    "$$softmax(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "scores = [0.7, 1.4, 3]\n",
    "print(softmax(scores))\n",
    "print(sum(softmax(scores)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning procedure aims at minimizing the loss function. It is chosen in conjunction with the task and the output units.\n",
    "\n",
    "For example, when we deal with a **regression** task we may choose *Mean Squared Error* loss (MSE).\n",
    "\n",
    "The most common cost function for **classification** is the *Cross-Entropy* between training data and model prediction. \n",
    "\n",
    "Consider for simplicity a binary classification problem: given $x$ the input example, $a$ the related output activation and $y$ the true label, the sum over the whole training set of size $N$, the cross-entropy is:\n",
    "\n",
    "$$C = -\\frac{1}{N}\\sum_{x}[{y\\ln(a) + (1 − y)\\ln(1 − a)}]$$\n",
    "\n",
    "When $y$ and $a$ have the same value, the cost function is zero.<br>\n",
    "When $y$ and $a$ have different value, the cost function of the single example becomes\n",
    "positive.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "During the forward step, the network accepts an input and computes the related output. In the figure at the top of this notebook, the information flows form left to right.\n",
    "This step produces a scalar cost that is the task-related cost function (e.g. the cross-entropy) plus, possibly, a regularization term (we will see it in the next notebook).\n",
    "## Backpropagation\n",
    "- check [this post](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "\n",
    "Gradient descent algorithm requires that the **gradient of the scalar cost is calculated with respect to the weights of the network**. This step is efficiently accomplished by the back-propagation algorithm, which originates from the chain rule of calculus.\n",
    "\n",
    "It provides an expression for the partial derivative $∂f(w)/∂w$ of the cost function $f$ with respect to any weight $w$ (or bias $b$) in the network: in other words, it expresses how a change in weights and biases affects the cost."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Based Learning\n",
    "When we train a neural network *from scratch*, the parameters (weights) of the network are randomly initialized. The optimization procedure aims to **minimize a cost function** by updating the weights in order to find a configuration that implements the desired function.\n",
    "\n",
    "Optimization is achieved using Gradient Descent, an iterative method from calculus. Given $f(x)$ the cost function we want to minimize by altering the parameters $x$, and given $ε$ the **learning rate** that defines the size of the step, the method updates the parameters at a new iteration using the following formula:\n",
    "$$∆x =- ε∇_x f (x) $$\n",
    "$$x ← x + ∆x$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back-propagation allows to compute the gradient of the cost function of a single example, but theoretically the average value over the entire training set is needed. In practice, the gradient in the formula can be evaluated by averaging over $n$ examples, and this value identifies three possible scenarios:\n",
    "- **online gradient descent**: optimization uses one example at a time; ($n = 1$)\n",
    "- **deterministic gradient descent**: all the training examples are used; ($n = $Training set size)\n",
    "- **minibatch** or **stochastic gradient descent**: a minibatch (or simply **batch**) of $n$ examples is used. ($1 < n < $Training set size)\n",
    "\n",
    "Stochastic gradient descent is a common choice in most practical applications: higher values of $n$ guarantee a better estimate of the gradient, while lower values imply lower computation time. Actually, there are several hardware-related issues: the computation over a batch of n examples can be performed more efficiently that $n$ computations of single examples thanks to parallel computing. On\n",
    "the other hand, big values of $n$ require a relevant amount of memory.\n",
    "\n",
    "When the network performs the forward and backward pass on a single batch of examples, it is said to have executed a **step** or an **iteration**. \n",
    "\n",
    "An **epoch** of training is composed by a number of steps that allow the network to see all the training examples.\n",
    "\n",
    "### Improving the optimization strategy \n",
    "\n",
    "Stochastic Gradient Descent is (\\*was\\*) the most popular method for optimization in DNN. Nevertheless several variants have been proposed in order to accelerate learning: **momentum** algorithm, for example, represents a common choice in convolutional neural networks. The update rule is:\n",
    "$$∆x = α∆x − ε∇ x f (x)$$\n",
    "$$x ← x + ∆x$$\n",
    "The only difference between this formulation and the classic SGD\n",
    "is given by the term $α∆x$: it can be considered a velocity term in the parameters space. The relationship between $α$ and $ε$ determines how much previous updates influence the current update, contributing to keep the direction of motion in the space parameter and avoiding oscillation. \n",
    "\n",
    "\n",
    "### Annealing the learning rate\n",
    "In practice, it is common to implement a form of learning rate decay: after an initial coarse gradient descent, we may want to reach and settle in the global minimum with smaller and smaller steps in the parameter space.\n",
    "\n",
    "Three popular implementations are step decay, exponential decay and $1/t$ decay.\n",
    "\n",
    "\n",
    "### Per-parameter adaptive learning rate methods\n",
    "The above-mentioned methods update the learning rate globally and equally for all the network parameters. Several algorithms have been proposed for adaptively tuning the learning rate per-parameters. Popular examples are **RMSprop** (Root Mean Square Propagation) and **Adam** (Adaptive Moment Estimation): they are already implemented and available in `tf.keras`.\n",
    "We will not go into the details of their behavior, but they are used frequently in practical applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "- Check Michael Nielsen online book: [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html )\n",
    "- Universal approximation framework\n",
    "  - [Neural Networks and Deep Learning - Chapter 4](http://neuralnetworksanddeeplearning.com/chap4.html)\n",
    "- Activation function\n",
    "  - more on ReLU: [Glorot, Xavier, Antoine Bordes, and Yoshua Bengio. \"Deep sparse rectifier neural networks.\" Proceedings of the fourteenth international conference on artificial intelligence and statistics. 2011.](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)\n",
    "- Optimizers:\n",
    "  - Check [this post](http://ruder.io/optimizing-gradient-descent/) for an overview of gradient descent optimization algorithms\n",
    "  - Check [this post](https://distill.pub/2017/momentum/) for an interactive explanation of Momentum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first Example: MNIST dataset\n",
    "- Chapter 2, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff).\n",
    "\n",
    "\n",
    "![img](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png) \n",
    "\n",
    "The problem is well-known: which digit is depicted in each image?\n",
    "\n",
    "- MNIST: Modified National Institute of Standards and Technology;\n",
    "- images of handwritten digits (28 pixels by 28 pixels);\n",
    "- 10-way classification problem (0 to 9);\n",
    "- 60,000 training images, plus 10.000 test images;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are the results stable?\n",
    "\n",
    "Training in Neural Networks is a stochastic procedure: it involves a number of random operations, such as random initialization of weights.\n",
    "\n",
    "As a consequence, if we train the same model twice, we will get different results.\n",
    "\n",
    "Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.random.randint(0,10,5))\n",
    "print(np.random.randint(0,10,5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can get reproducible results by setting the *seed*, i.e. a number used to initialize the pseudorandom number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "print(np.random.randint(0,10,5))\n",
    "print(np.random.randint(0,10,5))\n",
    "print()\n",
    "np.random.seed(123)\n",
    "print(np.random.randint(0,10,5))\n",
    "print(np.random.randint(0,10,5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, fixing the numpy source of randomness does not ensure reproducible behavior: we can still get different results because of randomness introduced by third-part libraries or by the usage of GPU.\n",
    "### Long Story Short\n",
    "- getting reproducible results using Keras with Tensorflow as Backend is arduous\n",
    "- currently, exact reproducibility:\n",
    "  - is not guaranteed on GPU\n",
    "  - requires a bit of coding effort and increased runtime on CPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_name =\"example\"\n",
    "if not os.path.exists(dir_name):\n",
    "  os.makedirs(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary for reproducible results of certain Python hash-based operations.\n",
    "os.environ[\"PYTHONHASHSEED\"]=\"0\"\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "np.random.seed(31)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "rn.seed(14)\n",
    "\n",
    "# The below tf.random.set_seed will make random number generation in TensorFlow have a well-defined initial state.\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However:\n",
    "> *when running on a GPU, some operations have non-deterministic outputs, in particular tf.reduce_sum(). This is due to the fact that GPUs run many operations in parallel, so the order of execution is not always guaranteed. Due to the limited precision of floats, even adding several numbers together may give slightly different results depending on the order in which you add them.*\n",
    "\n",
    "The above mentioned tricks reduce the impact of most non-deterministic operations, but some others may be created automatically by TensorFlow to compute the gradients when using GPU.\n",
    "To date, the only safe way to get exactly reproducible results is to set the random seeds AND run the code on the CPU. For this, you can set the `CUDA_VISIBLE_DEVICES` environment variable to an empty string, or disable the GPU in colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "print(keras.__version__)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape,train_labels.shape)\n",
    "print(test_images.shape,test_labels.shape)\n",
    "print(train_images.dtype)\n",
    "print(test_images.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_labels)\n",
    "plt.show()\n",
    "#to check if the dataset is well balanced or not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use matplotlib to plot the first training sample:\n",
    "plt.imshow(train_images[19,:,:],cmap = 'gray')\n",
    "plt.title('first sample')\n",
    "print('True Label is',train_labels[19])\n",
    "# print(train_images[0,:,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameters vs Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to build a model we have to carefully design its components: how many hidden layers? how many units per layer? Which optimizer and learning rate value?\n",
    "\n",
    "Indeed, the behaviour of an algorithm for training a neural network is affected\n",
    "by a set of **hyperparameters**. Examples of hyperparameters are:\n",
    "- Number of layers and hidden units: affects the capacity of the model;\n",
    "- Learning rate: determines the step size in learning procedure;\n",
    "- Learning rate decay strategy;\n",
    "- Mini-batch size;\n",
    "- Number of epochs of training and stop criterion;\n",
    "- Weights initialization strategy;\n",
    "- Preprocessing strategy;\n",
    "- and possibly many others\n",
    "\n",
    "They are defined *hyperparameters*, as opposed to the *parameters* of a network, i.e. weights and biases that define the behaviour of neurons and are adjusted by the training procedure. That is why *parameters* are referred to as *trainable parameters*.\n",
    "\n",
    "Defining the correct hyperparameter configuration is of the utmost importance, and it may take days or weeks of tuning. Indeed, an active area of research is *hyperparameter optimization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of output units and their activation is determined by our problem: ten digits\n",
    "output_units = 10\n",
    "output_activation = 'softmax'\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_units = 512\n",
    "hidden_activation = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(hidden_units, activation=hidden_activation, input_shape=(28 * 28,))) # we need to specify the input shape\n",
    "network.add(layers.Dense(output_units, activation=output_activation))\n",
    "network.summary() # pay attention to the number of parameters. where do they come from?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our network ready for training, we need to set three more things:\n",
    "\n",
    "* a loss function;\n",
    "* an optimizer;\n",
    "* metrics to monitor during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='sgd', # rmsprop (Root Mean Square Propagation) is a good choice for the optimizer\n",
    "                loss='sparse_categorical_crossentropy', # categorical crossentropy for a multi-class clasification problem\n",
    "                metrics=['accuracy']) # accuracy = number of correctly labeled samples / number of samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n",
    "Our network is a MLP: we need to reshape images into a vector of input units.\n",
    "\n",
    "Remember that our network expects a 2D tensor of input shape of (Batch_size, 28*28=784) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "test_images = test_images.reshape((10000, 28 * 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape # check the shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "print(train_images[0].dtype)\n",
    "plt.hist(train_images[0])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other hyperparameters\n",
    "epochs = 10 # how many times the network experiments the whole training set\n",
    "batch_size = 128 #how many examples the network exmperiments at every step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = network.fit(train_images, \n",
    "            train_labels, \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history.history is a dictionary\n",
    "loss = history.history[\"loss\"]\n",
    "acc = history.history[\"acc\"]\n",
    "print('loss',loss)\n",
    "print('acc',acc)\n",
    "plt.figure()\n",
    "plt.plot(loss,'^-b')\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(acc,'^-b')\n",
    "plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)\n",
    "\n",
    "predictions = network.predict(test_images) # get the output for each sample\n",
    "predicted_lab = np.argmax(predictions,axis=1) # from one-hot encoding to integer\n",
    "\n",
    "predicted_lab.shape,test_labels\n",
    "\n",
    "correct_predictions = np.sum(np.equal(predicted_lab,test_labels))\n",
    "accuracy = correct_predictions/len(test_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the trained model, we will compare its performance with another, more sofisticated, model\n",
    "network.save(os.path.join(dir_name,\"MNIST_MLP.h5\"))  # creates a HDF5 file 'my_model.h5'\n",
    "del network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fighting Overfitting\n",
    "The most common ways to prevent overfitting in neural networks are:\n",
    "- **Getting more training data**: a model trained on more (quality) data will naturally generalize better. Gathering more data may be even more useful than improving the learning algorithm.\n",
    "\n",
    "- **Performing dataset augmentation**: The restricted size of available databases is one of the central issue in many machine learning applications. Gathering and annotating new data is often a difficult task. A solution, particularly adopted with images, is to introduce several types of transformation or distortion that consist in an artificial dataset augmentation. \n",
    "\n",
    "- **Reducing the capacity of the network**: Overfitting is the use of models or procedures that violate [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor) (paraphrasable as: *The simplest solution is most likely the right one*) for example by uselessly including a high number of adjustable parameters, or by using a more complicated approach than the one is ultimately optimal. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the patterns that have a better chance of generalizing well.\n",
    "\n",
    "- **Adding weight regularization**: this technique consists in forcing the weights of a network to only take small values: it is achieved by adding a parameter norm penalty $\\theta(w)$ (associated with having large weights) to the loss function of the network $J(w)$. As a consequentce: $J(w) = J(w)+\\lambda\\theta(w)$. For example:\n",
    "  - with *L2 regularization*, the penalty is proportional to the squared value of the weights coefficients\n",
    "  - with *L1 regularization*, the penalty is proportional to the absolute value of the weights coefficients\n",
    "\n",
    "- **Adding dropout**: this technique is widely used to reduce overfitting in neural network: when dropout is applied to a layer, at every training step every unit of that layer is kept with probability p, otherwise it is temporarily ignored. During evaluation (i.e. inference on validation or test set) the dropout is turned-off and all the\n",
    "neurons are kept.  Dropout effect could be seen from two points of view. On one hand, by randomly sampling the neurons to delete at every step, it reduces the co-adaptation phenomenon between neurons; on the other hand, it results in a procedure that trains a variety of different neural networks (i.e. different number of units per layer). These subnetworks are supposed to overfit the training set in different way, so the global network should be able to generalize better.<br>\n",
    "![dropout](https://static.commonlounge.com/fp/600w/aOLPWvdc8ukd8GTFUhff2RtcA1520492906_kc)\n",
    "<br>\n",
    "\n",
    "[Geoffry Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) - The idea behind dropout: \"*I went to my bank. The tellers kept changing and I asked one of them why. He said he didn’t know but they got moved around a lot. I figured it must be because it would require cooperation between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting.*\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Try to vary the hyperparameter of your model\n",
    "- number of layers / units per layer\n",
    "- optimizer learning rate\n",
    "- dropout rate (https://keras.io/api/layers/regularization_layers/dropout/)\n",
    "- Add L1 or L2 regularization\n",
    "  - https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense  \n",
    "  - https://www.tensorflow.org/api_docs/python/tf/keras/regularizers\n",
    "\n",
    "\n",
    "Share your best model (highest final test accuracy) with the class!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for 5G DDoS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "INDEX = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "CICIDS-2017\n",
    "A dataset that contains labeled network flows, both benign and attack packets.\n",
    "CICIDS2017 dataset contains benign and the most up-to-date common attacks, which resembles the true real-world data (PCAPs). It also includes the results of the network traffic analysis using CICFlowMeter with labeled flows based on the time stamp, source, and destination IPs, source and destination ports, protocols and attack (CSV files) **we are going to use CSV file**. Also available is the extracted features definition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data capturing period started at 9 a.m., Monday, July 3, 2017 and ended at 5 p.m. on Friday July 7, 2017, for a total of 5 days. Monday is the normal day and only includes the benign traffic. The implemented attacks include Brute Force FTP, Brute Force SSH, DoS, Heartbleed, Web Attack, Infiltration, Botnet and DDoS. They have been executed both morning and afternoon on Tuesday, Wednesday, Thursday and Friday."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eleven criterias satisfied by the dataset:\n",
    "- **Complete Network configuration**: A complete network topology includes Modem, Firewall, Switches, Routers, and presence of a variety of operating systems such as Windows, Ubuntu and Mac OS X.\n",
    "- **Complete Traffic**: By having a user profiling agent and 12 different machines in Victim-Network and real attacks from the Attack-Network.\n",
    "- **Labelled Dataset**: Dataset is labelled\n",
    "- **Complete Interaction**:  covered both within and between internal LAN by having two different networks and Internet communication as well.\n",
    "- **Complete Capture**: all traffics have been captured and recorded on the storage server.\n",
    "- **Available Protocols**: Provided the presence of all common available protocols, such as HTTP, HTTPS, FTP, SSH and email protocols.\n",
    "- **Attack Diversity**: Included the most common attacks based on the 2016 McAfee report, such as Web based, Brute force, DoS, DDoS, Infiltration, Heart-bleed, Bot and Scan covered in this dataset.\n",
    "- **Heterogeneity**: Captured the network traffic from the main Switch and memory dump and system calls from all victim machines, during the attacks execution.\n",
    "- **Feature Set**: Extracted more than 80 network flow features from the generated network traffic using CICFlowMeter and delivered the network flow dataset as a CSV file. See our PCAP analyzer and CSV generator.\n",
    "- **MetaData**: Completely explained the dataset which includes the time, attacks, flows and labels in the published paper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_data (folder, file_name, index=INDEX):\n",
    "    '''\n",
    "        Read data file from specific data folder. \n",
    "        \n",
    "        Returns:\n",
    "            data: list of features extracted from the file.\n",
    "    '''\n",
    "    data = []\n",
    "\n",
    "    try:\n",
    "        print(folder+file_name)\n",
    "        dataset = pd.read_csv(folder+file_name)\n",
    "\n",
    "        dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        dataset.dropna(inplace=True)\n",
    "\n",
    "        print(dataset.agg([min, max]))\n",
    "        \n",
    "        X = dataset.iloc[:,:-1].to_numpy()\n",
    "        #print(X)\n",
    "        y = dataset.iloc[:,-1].to_numpy()\n",
    "        #print(X.shape)\n",
    "        \n",
    "        X, _, y , _ = train_test_split(X, y, train_size=45000, random_state=index, stratify=y)\n",
    "        \n",
    "        \n",
    "\n",
    "        return X, y\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file!\")\n",
    "        return None        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y  = load_file_data (folder=data_path, file_name = \"data.csv\")\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "print(np.unique(y,return_counts=True))\n",
    "\n",
    "plt.pie(np.unique(y,return_counts=True)[1], labels = np.unique(y,return_counts=True)[0], autopct = '%.0f%%', radius= 1.5, textprops={'fontsize': 16})\n",
    "plt.show()\n",
    "# need to balance the dataset!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinMaxScaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before normalization\")\n",
    "print(X)\n",
    "scaler = MinMaxScaler((-1,1))\n",
    "X = scaler.fit_transform(X)\n",
    "print(\"After normalization\")\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "\n",
    "We need to split the data in training/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=INDEX, stratify=y)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to balance **ONLY** the training set, the test set must resemble as much as possible real-world data!\n",
    "\n",
    "Choose one technique:\n",
    "- Oversampling\n",
    "- Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling data\n",
    "print(\"Data before balancing\")\n",
    "print(np.unique(y_train,return_counts=True))\n",
    "\n",
    "# define oversampling strategy\n",
    "sampler = RandomOverSampler(sampling_strategy='minority')\n",
    "X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Data after balancing\")\n",
    "print(np.unique(y_train,return_counts=True))\n",
    "plt.pie(np.unique(y_train,return_counts=True)[1], labels = np.unique(y_train,return_counts=True)[0], autopct = '%.0f%%', radius= 1.5, textprops={'fontsize': 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling data\n",
    "print(\"Data before balancing\")\n",
    "print(np.unique(y_train,return_counts=True))\n",
    "# define undersampling strategy\n",
    "undersampler = RandomUnderSampler()\n",
    "X_train, y_train = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Data after balancing\")\n",
    "print(np.unique(y_train,return_counts=True))\n",
    "plt.pie(np.unique(y_train,return_counts=True)[1], labels = np.unique(y_train,return_counts=True)[0], autopct = '%.0f%%', radius= 1.5, textprops={'fontsize': 16})\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "epochs =  # how many times the network experiments the whole training set\n",
    "batch_size =  #how many examples the network exmperiments at every step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = to_categorical(y_train)\n",
    "#print(y_train)\n",
    "\n",
    "# Define your model here\n",
    "# model hyperparameters\n",
    "\n",
    "epochs = 10 # how many times the network experiments the whole training set\n",
    "batch_size = 32 #how many examples the network exmperiments at every step\n",
    "\n",
    "\n",
    "# MODEL \n",
    "hidden_units = 1024\n",
    "output_units = 2\n",
    "output_activation = 'softmax'\n",
    "hidden_activation = 'relu'\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(256, activation=hidden_activation, input_shape=(78,))) # we need to specify the input shape\n",
    "#network.add(layers.Dropout(0.2))\n",
    "#network.add(layers.Dense(1024, activation=hidden_activation))\n",
    "network.add(layers.Dense(128, activation=hidden_activation))\n",
    "#network.add(layers.Dense(100, activation='relu'))\n",
    "network.add(layers.Dense(output_units, activation=output_activation))\n",
    "network.summary() # pay attention to the number of parameters. where do they come from?\n",
    "\n",
    "\n",
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "\n",
    "print(y_train.shape)\n",
    "# compile model \n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "network.compile(optimizer=opt, # Adam is a good choice for the optimizer\n",
    "                loss='sparse_categorical_crossentropy', # sparse categorical crossentropy \n",
    "                metrics=['accuracy']) # accuracy = number of correctly labeled samples / number of samples\n",
    "\n",
    "\n",
    "# fit model here\n",
    "history = network.fit(X_train, \n",
    "            y_train, \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the models and report scores\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report confusion matrix, accuracy and loss history \n",
    "!python3 -m pip install seaborn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "labels = [0,1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "labels = [0,1]\n",
    "\n",
    "# test the models and report scores\n",
    "print(np.unique(y_test,return_counts=True))\n",
    "\n",
    "y_prob = network.predict(X_test) \n",
    "print(y_prob)\n",
    "y_classes = np.argmax(y_prob, axis=1)\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_classes)\n",
    "accuracy = accuracy_score(y_test, y_classes)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_classes))\n",
    "print(confusion_matrix(y_test,y_classes))\n",
    "\n",
    "# report metrics\n",
    "# history.history is a dictionary\n",
    "loss = history.history[\"loss\"]\n",
    "acc = history.history[\"accuracy\"]\n",
    "print('loss',loss)\n",
    "print('accuracy',acc)\n",
    "plt.figure()\n",
    "plt.plot(loss,'^-b')\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(acc,'^-b')\n",
    "plt.title('Accuracy')\n",
    "plt.show()\n",
    "#ConfusionMatrixDisplay(confusion_matrix(y_test,y_classes), display_labels=labels).plot()\n",
    "\n",
    "print(\"F1-score: \", f1)\n",
    "print(\"Accuracy score: \", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
